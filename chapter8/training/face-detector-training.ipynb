{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53707c8-a9f8-401c-a06d-d93283538e59",
   "metadata": {},
   "source": [
    "# Image Classification Training Notebook\n",
    "\n",
    "This notebook is used for training an EfficientNet model for classifying a human face, a human finger and a background image. This was created for demo purposes where a user testing the model can use a webcam, hence the choices of classed *(finger, face and background/nothing)*.\n",
    "\n",
    "This notebook requires TensorFlow 2.11.X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009aee22-4de3-4efe-b6e0-1f9a06c106ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, sys; sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57a251-2665-4694-991a-48ebe32a77f9",
   "metadata": {},
   "source": [
    "## Extract Training and Test Data\n",
    "This extracts training and validation data from data.zip and extracts test data from test.zip onto /data and /test respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b3293-4de9-4b49-8b58-bb5c36015ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = './MLOps-with-Red-Hat-OpenShift/chapter8/training/'\n",
    "data_path = current_dir + 'data/'\n",
    "compressed_data_path = current_dir + 'data.zip'\n",
    "test_data_path = current_dir + 'test/'\n",
    "compressed_test_data_path = current_dir + 'test.zip'\n",
    "\n",
    "def extract_zip(compressed_path, dest_path):\n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(dest_path):\n",
    "        # Use shutil.rmtree() to delete the folder and its contents\n",
    "        shutil.rmtree(dest_path)\n",
    "        print(f\"The folder '{dest_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"The folder '{dest_path}' does not exist.\")\n",
    "\n",
    "    with zipfile.ZipFile(compressed_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_path)\n",
    "\n",
    "    print(f\"The file {compressed_path} was extracted to {dest_path}\")\n",
    "\n",
    "extract_zip(compressed_data_path, data_path)\n",
    "extract_zip(compressed_test_data_path, test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ca4cf-c58c-4fb1-850e-357e120ea14f",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "This step prepares the training and validation datasets. The images in ./data is split into two groups \"training\" and \"validation\" where 20% of the images is used in the \"validation\" group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91357682-3a49-4019-b280-288600c85cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256, 256)\n",
    "batch_size = 4\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\"\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\"\n",
    ")\n",
    "\n",
    "NUM_CLASSES = len(train_ds.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b675509-6d5a-43cd-8a14-6c44a50092fd",
   "metadata": {},
   "source": [
    "## Preview Training Dataset\n",
    "\n",
    "Verify that we have correctly loaded the images into the training dataset **train_ds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466d8ca-bcd9-4479-aa96-950a9d57c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(4):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59959f-7986-4875-ba8c-3f1cdd06d94d",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "Enrich the training data by providing flipped and rotated version of the images.\n",
    "\n",
    "When you don't have a large image dataset, it's a good practice to artificially\n",
    "introduce sample diversity by applying random yet realistic transformations to the\n",
    "training images, such as random horizontal flipping or small random rotations. This\n",
    "helps expose the model to different aspects of the training data while slowing down overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df9731-5e1c-4197-9cc5-080f9c27363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize augmented data\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(8):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a222e-f386-4e13-96a2-6df6362ed24f",
   "metadata": {},
   "source": [
    "## Optimize Dataset Performance\n",
    "\n",
    "Ensure the use buffered prefetching and pre-processing so to reduce disk I/O operations during training.\n",
    "This is particularly important for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a8cdd-a7b9-4168-9f21-0aa7a9bde63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot / categorical encoding\n",
    "def input_preprocess(image, label):\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "    input_preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(input_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34545fb-790e-45df-9000-754861285c69",
   "metadata": {},
   "source": [
    "## Configure the Model Architecture\n",
    "\n",
    "We'll build a small version of the Xception network. We haven't particularly tried to\n",
    "optimize the architecture; if you want to do a systematic search for the best model\n",
    " configuration, consider using\n",
    "[KerasTuner](https://github.com/keras-team/keras-tuner).\n",
    "\n",
    "Note that:\n",
    "- We start the model with the `data_augmentation` preprocessor, followed by a\n",
    " `Rescaling` layer.\n",
    "- We also include a `Dropout` layer before the final classification layer.\n",
    "\n",
    "Note: You may optionally visualize the resulting model architecture by adding the following line of code. This requires pydot which you can install by running the command: `!pip install pydot`\n",
    "\n",
    "`keras.utils.plot_model(model, show_shapes=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b8030-bd91-4e2d-8440-4b38266ce59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Image augmentation block\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    model = EfficientNetV2B0(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = True\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=image_size + (3,), num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9063364-cfb0-4f8a-9d87-066f4b5225c2",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This compiles the model and starts the model training process.\n",
    "You can play around witht the parameters such use the number of epochs and the optimizer used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f152d98-673a-4004-83b6-7d657f388e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"../deploy/model.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"min\",\n",
    "    save_freq=\"epoch\",\n",
    "\n",
    ")\n",
    "callbacks = [\n",
    "    save_best,\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8006e2-6aa3-457b-8016-e8d23b20380c",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now that we have a model, lest use it to perform some inferences. There `/test` directory contains images that we will use to test.\n",
    "\n",
    "This step will go run a prediction for each of the images in the `/test` directory. The files are names according to their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc108d-51b6-4f09-a366-18621f10a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_class(score):\n",
    "    number = np.argmax(score)\n",
    "    words = [\"Background\", \"Finger\", \"Face\"]\n",
    "    if 0 <= number <= 2:\n",
    "        return words[number]\n",
    "    else:\n",
    "        return \"Invalid input\"\n",
    "    \n",
    "\n",
    "for filename in os.listdir(test_data_path):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        # Do something with the file (for example, print the file name)\n",
    "        print(f\"test for --> {filename}\")\n",
    "\n",
    "        img = keras.preprocessing.image.load_img(\n",
    "            test_data_path+filename, target_size=image_size, color_mode=\"rgb\"\n",
    "        )\n",
    "\n",
    "        img_array = keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "        predictions = model.predict(img_array)\n",
    "        score = predictions[0]\n",
    "\n",
    "        print(score)\n",
    "        print(\"prediction --> \" + get_class(score))\n",
    "        print(\"#####################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46d432-105f-4b23-b732-19245234445f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
